"""SatyaAI Python-First FastAPI Application"""

import logging
import os
import sys
import time
import asyncio
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from typing import Any, Dict, List, Optional, Tuple, Union

load_dotenv()
load_dotenv('../../.env')

# Force CPU-only mode to fix CUDA compatibility issues
os.environ['ENABLE_ML_MODELS'] = 'true'
os.environ['FORCE_ML_LOADING'] = 'true'
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Disable CUDA
os.environ['TORCH_CUDA_ARCH_LIST'] = ''  # Disable CUDA arch list
import torch
# Let PyTorch handle CUDA naturally - don't force it

from fastapi import FastAPI, HTTPException, Request, status, UploadFile, File, Depends
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware

sys.path.append(str(Path(__file__).parent.parent))

if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", errors="replace")

handlers = [logging.StreamHandler()]
logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO").upper() if os.getenv("LOG_LEVEL") else "INFO",
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=handlers,
)
logger = logging.getLogger(__name__)

from config import Settings

try:
    settings = Settings()
    logger.info("‚úÖ Configuration loaded successfully")
except Exception as e:
    logger.error(f"‚ùå Failed to load configuration: {e}")
    logger.error("Please check your environment variables and .env file")
    sys.exit(1)

try:
    from services.cache import CacheManager
    CACHE_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Cache service not available: {e}")
    CACHE_AVAILABLE = False
    CacheManager = None

try:
    from services.database import DatabaseManager
    DATABASE_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Database service not available: {e}")
    DATABASE_AVAILABLE = False
    DatabaseManager = None

try:
    from services.model_preloader import model_preloader
    PRELOADER_AVAILABLE = True
    logger.info("‚úÖ Model preloader available")
except ImportError as e:
    logger.warning(f"Model preloader not available: {e}")
    PRELOADER_AVAILABLE = False
    model_preloader = None

try:
    from services.error_recovery import error_recovery
    ERROR_RECOVERY_AVAILABLE = True
    logger.info("‚úÖ Error recovery service available")
except ImportError as e:
    logger.warning(f"Error recovery service not available: {e}")
    ERROR_RECOVERY_AVAILABLE = False
    error_recovery = None

# WebSocket is handled by Node.js backend - not needed in Python

# Node.js handles WebSocket at /api/v2/dashboard/ws

WEBSOCKET_AVAILABLE = False

WebSocketManager = None


# Import CUDA fixes for Windows
try:
    from cuda_fix import apply_cuda_fixes
    CUDA_FIX_AVAILABLE = True
except ImportError:
    CUDA_FIX_AVAILABLE = False
    logger.warning("CUDA fix module not available - GPU detection may be limited")

# Apply CUDA fixes early
if CUDA_FIX_AVAILABLE:
    try:
        cuda_available = apply_cuda_fixes()
        logger.info(f"üîß CUDA fixes applied - GPU available: {cuda_available}")
    except Exception as e:
        logger.error(f"‚ùå Failed to apply CUDA fixes: {e}")

# Set DB_AVAILABLE based on database service only (remove cache dependency)
DB_AVAILABLE = DATABASE_AVAILABLE


# Import middleware

try:

    from middleware.auth_middleware import SupabaseAuthMiddleware, get_current_user

    from middleware.rate_limiter import RateLimitMiddleware

    MIDDLEWARE_AVAILABLE = True

except ImportError as e:

    logger.warning(f"Middleware not available: {e}")

    MIDDLEWARE_AVAILABLE = False


# Import ML services

# Force enable ML models for development

ENABLE_ML_MODELS = True

ENABLE_ADVANCED_MODELS = True

ENABLE_ML_OPTIMIZATION = True

STRICT_MODE_ENABLED = True


# Force enable all ML models via environment

os.environ['ENABLE_ML_MODELS'] = 'true'

os.environ['ENABLE_ADVANCED_MODELS'] = 'true'

os.environ['ENABLE_ML_OPTIMIZATION'] = 'true'

os.environ['STRICT_MODE_ENABLED'] = 'true'


# Initialize ML_AVAILABLE flag

ML_AVAILABLE = False


if ENABLE_ML_MODELS:

    try:

        # Test basic torch import first

        import torch

        logger.info(f"‚úÖ PyTorch available: {torch.__version__}")

        
        # Now try to import detectors

        from detectors.audio_detector import AudioDetector

        from detectors.image_detector import ImageDetector

        from detectors.video_detector import VideoDetector

        from detectors.text_nlp_detector import TextNLPDetector

        from detectors.multimodal_fusion_detector import MultimodalFusionDetector

        ML_AVAILABLE = True

        logger.info("‚úÖ All ML detectors imported successfully")

    except ImportError as e:

        logger.warning(f"ML services not available: {e}")

        ML_AVAILABLE = False

    except Exception as e:

        logger.error(f"ML initialization failed: {e}")

        ML_AVAILABLE = False

else:

    logger.warning("ML models disabled by ENABLE_ML_MODELS flag")

    ML_AVAILABLE = False


# Import routers - ONLY analysis and health per constraints
try:
    from routes.analysis import router as analysis_router
    from routes.health import router as health_router

    ROUTES_AVAILABLE = True
except ImportError as e:
    ROUTES_AVAILABLE = False


# Import unified detector for consistent interface

try:

    from detectors.unified_detector import get_unified_detector, ModalityType

    UNIFIED_DETECTOR_AVAILABLE = True

    logger.info("‚úÖ Unified detector with consistent interface available")

except ImportError as e:

    logger.warning(f"Unified detector not available: {e}")

    UNIFIED_DETECTOR_AVAILABLE = False

    get_unified_detector = None

    ModalityType = None

except Exception as e:

    logger.warning(f"Unified detector initialization failed: {e}")

    UNIFIED_DETECTOR_AVAILABLE = False

    get_unified_detector = None

    ModalityType = None

try:

    from satyaai_core import SatyaAICore, DetectorType

    SATYAAI_CORE_AVAILABLE = True

    logger.info("‚úÖ SatyaAI Core with multi-modal fusion available")

except ImportError as e:

    logger.warning(f"SatyaAI Core not available: {e}")

    SATYAAI_CORE_AVAILABLE = False

    SatyaAICore = None

    DetectorType = None


# Import monitoring

try:

    from prometheus_fastapi_instrumentator import Instrumentator


    PROMETHEUS_AVAILABLE = True

except ImportError:

    logger.warning("Prometheus not available")

    PROMETHEUS_AVAILABLE = False


# Lifespan context manager for startup/shutdown

@asynccontextmanager

async def lifespan(app: FastAPI):

    """Manage application lifespan"""

    # Declare global variables to fix UnboundLocalError

    global ML_AVAILABLE, DB_AVAILABLE

    
    logger.info("üöÄ Starting SatyaAI Python API Server...")


    # Startup

    try:

        # Preload ML models for faster first requests

        if ML_AVAILABLE and PRELOADER_AVAILABLE:

            try:

                logger.info("ü§ñ Preloading ML models...")

                preload_results = await model_preloader.preload_all_models()

                
                success_count = sum(1 for success in preload_results.values() if success)

                total_count = len(preload_results)

                
                if success_count > 0:

                    logger.info(f"‚úÖ Preloaded {success_count}/{total_count} model types")

                    summary = model_preloader.get_preload_summary()

                    logger.info(f"üìä Preload summary: {summary}")

                else:

                    logger.warning("‚ö†Ô∏è No models were preloaded")

                    
            except Exception as e:

                logger.error(f"‚ùå Model preloading failed: {e}")

                logger.info("üîÑ Models will be loaded on-demand instead")

        
        # Initialize database

        if DB_AVAILABLE:

            try:

                logger.info("üì¶ Initializing database...")

                from services.database import get_db_manager


                db_manager = get_db_manager()


                # Enhanced database connection test with retry logic

                max_retries = 5
                for attempt in range(max_retries):
                    try:
                        # Test connection with timeout
                        connection_result = await asyncio.wait_for(
                            db_manager.test_connection(), 
                            timeout=30.0  # 30 second timeout
                        )
                        break  # Success, exit retry loop
                    except Exception as e:
                        logger.error(f"‚ùå Database initialization failed: {e}")
                        logger.warning("‚ö†Ô∏è Continuing without database - some features may be limited")
                        if attempt == max_retries - 1:
                            logger.warning("‚ö†Ô∏è Max retries reached, continuing without database")
                        break

            except Exception as e:
                logger.error(f"‚ùå Database setup failed: {e}")
                logger.warning("‚ö†Ô∏è Continuing without database - some features may be limited")


        # Load ML models (CRITICAL - must succeed for production)

        if ML_AVAILABLE:

            logger.info("ü§ñ Loading ML models - CRITICAL for production...")
            
            # Initialize SentinelAgent with all detectors
            try:
                from sentinel_agent import SentinelAgent
                app.state.sentinel_agent = SentinelAgent()
                
                # Force initialize image detector for testing
                from services.detector_singleton import get_detector_singleton
                detector_singleton = get_detector_singleton()
                image_detector = detector_singleton.get_detector('image', {
                    'enable_gpu': False
                })
                
                # WARNING: Model validation for testing - allow partial initialization
                model_status = {
                    'image': hasattr(app.state.sentinel_agent, 'image_detector') and app.state.sentinel_agent.image_detector,
                    'video': hasattr(app.state.sentinel_agent, 'video_detector') and app.state.sentinel_agent.video_detector,
                    'audio': hasattr(app.state.sentinel_agent, 'audio_detector') and app.state.sentinel_agent.audio_detector
                }
                
                # Log model status but don't fail for testing
                for model_type, status in model_status.items():
                    if status:
                        logger.info(f"‚úÖ {model_type.capitalize()} detector initialized")
                    else:
                        logger.warning(f"‚ö†Ô∏è {model_type.capitalize()} detector failed to initialize - continuing for testing")
                
                logger.info("‚úÖ SentinelAgent initialized - some models may be lazy loaded")

            except Exception as e:
                logger.error(f"‚ö†Ô∏è ML model initialization warning: {e}")
                logger.warning("‚ö†Ô∏è Continuing without full ML model support - models will load on demand")

            
            # Initialize unified detector for consistent interface (non-critical)
            if UNIFIED_DETECTOR_AVAILABLE and not hasattr(app.state, 'unified_detector'):
                try:
                    import torch  # Import torch here for availability check
                    config = {
                        "MODEL_PATH": "models",
                        "ENABLE_GPU": torch.cuda.is_available(),
                        "ENABLE_FORENSICS": True,
                        "ENABLE_MULTIMODAL": True
                    }
                    app.state.unified_detector = get_unified_detector(config)
                    logger.info("‚úÖ Unified detector initialized with consistent interface")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Unified detector initialization failed (non-critical): {e}")
                    app.state.unified_detector = None

            
            # Initialize SatyaAI Core with multi-modal fusion (only if unified detector not available)
            if SATYAAI_CORE_AVAILABLE and not hasattr(app.state, 'satyaai_core'):
                try:
                    import torch
                    config = {
                        "MODEL_PATH": "models",
                        "ENABLE_GPU": torch.cuda.is_available(),
                        "ENABLE_SENTINEL": True,
                        "MAX_WORKERS": 4,
                        "CACHE_RESULTS": True
                    }
                    app.state.satyaai_core = SatyaAICore(config)
                    logger.info("‚úÖ SatyaAI Core initialized with multi-modal fusion")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è SatyaAI Core initialization failed (non-critical): {e}")
                    app.state.satyaai_core = None

            
            # Only initialize model placeholders if ML is still available
            if ML_AVAILABLE:
                # Models are already loaded via SentinelAgent
                logger.info("‚úÖ All ML models loaded via SentinelAgent")
        else:
            logger.warning("‚ö†Ô∏è ML models not available - analysis features disabled")
            # Ensure app state has ML flags set to False
            app.state.sentinel_agent = None
            app.state.satyaai_core = None
            app.state.unified_detector = None
            app.state.image_detector = None
            app.state.video_detector = None
            app.state.audio_detector = None
            app.state.text_nlp_detector = None
            app.state.multimodal_detector = None
        
        # Initialize cache
        if DB_AVAILABLE:
            logger.info("üíæ Initializing cache...")
            try:
                from services.cache import CacheManager
                app.state.cache = CacheManager()
                logger.info("‚úÖ Cache initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Cache initialization failed: {e}")

        
        logger.info("‚úÖ SatyaAI API Server started successfully")

    except Exception as e:
        logger.error(f"‚ùå Failed to start server: {e}", exc_info=True)
        raise

    yield

    # Shutdown
    logger.info("üõë Shutting down SatyaAI API Server...")
    try:
        # Cleanup resources
        if DB_AVAILABLE:
            logger.info("Closing database connections...")
            await db_manager.disconnect()
        logger.info("‚úÖ Server shutdown complete")
    except Exception as e:
        logger.error(f"Error during shutdown: {e}", exc_info=True)


# Initialize FastAPI app with lifespan (single instance)
app = FastAPI(
    title="SatyaAI Internal API",
    docs_url="/api/docs",  # Enable Swagger UI at /api/docs
    redoc_url="/api/redoc",  # Enable ReDoc at /api/redoc
    description="Python-First REST API for deepfake detection using ML/DL/NLP",
    version="2.0.0",
    lifespan=lifespan,
)

# ============================================================================
# MIDDLEWARE CONFIGURATION
# ============================================================================

# CORS Configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ALLOW_ORIGINS,
    allow_credentials=settings.CORS_CREDENTIALS,
    allow_methods=settings.CORS_ALLOW_METHODS,
    allow_headers=settings.CORS_ALLOW_HEADERS,
    expose_headers=settings.CORS_EXPOSE_HEADERS,
    max_age=settings.CORS_MAX_AGE,
)

# Handle OPTIONS method for CORS preflight for all routes
@app.options("/{full_path:path}")
async def preflight_handler(request: Request, full_path: str):
    response = JSONResponse(
        content={"status": "ok"},
        status_code=200
    )
    origin = request.headers.get("Origin")
    if origin in settings.CORS_ALLOW_ORIGINS or any(
        origin and origin.startswith(domain) 
        for domain in ["http://localhost", "https://satyaai.app"]
    ):
        response.headers["Access-Control-Allow-Origin"] = origin
    response.headers["Access-Control-Allow-Methods"] = ", ".join(settings.CORS_ALLOW_METHODS)
    response.headers["Access-Control-Allow-Headers"] = ", ".join(h for h in settings.CORS_ALLOW_HEADERS if h != "*")
    response.headers["Access-Control-Allow-Credentials"] = "true"
    response.headers["Access-Control-Max-Age"] = str(settings.CORS_MAX_AGE)
    response.headers["Vary"] = "Origin"
    return response

# Add browser blocking middleware (commented out for testing)
# app.add_middleware(BlockBrowserMiddleware)


# Add Supabase auth middleware if available - TEMPORARILY DISABLED FOR TESTING - RESTART TRIGGER

if MIDDLEWARE_AVAILABLE and settings.SUPABASE_URL and settings.SUPABASE_ANON_KEY:

    try:

        app.add_middleware(

            SupabaseAuthMiddleware,

            supabase_url=settings.SUPABASE_URL,

            supabase_anon_key=settings.SUPABASE_ANON_KEY,

            public_paths=["/api/v2/docs", "/api/v2/redoc", "/health", "/", "/api/v2/analysis", "/analyze", "/api/v2/analysis/unified", "/api/v2/analysis/unified/image", "/api/v2/analysis/unified/video", "/api/v2/analysis/unified/audio"]

        )

        logger.info(" Supabase auth middleware enabled")

    except Exception as e:

        logger.error(f"Failed to add Supabase auth middleware: {e}")

else:

    logger.warning(" Supabase auth middleware disabled for testing")


# Add rate limiting middleware if available

if MIDDLEWARE_AVAILABLE:

    try:

        from middleware.rate_limiter import RateLimitMiddleware

        app.add_middleware(RateLimitMiddleware)

        logger.info("‚úÖ Rate limiting middleware enabled")

    except Exception as e:

        logger.error(f"Failed to add rate limiting middleware: {e}")


# Compression - Compress responses > 1KB

app.add_middleware(GZipMiddleware, minimum_size=1000, compresslevel=6)


# Trusted hosts (security)

app.add_middleware(

    TrustedHostMiddleware, allowed_hosts=["localhost", "127.0.0.1", "*.satyaai.com"]

)


# ============================================================================

# CUSTOM MIDDLEWARE

# ============================================================================


@app.middleware("http")

async def add_process_time_header(request: Request, call_next):

    """Add processing time to response headers"""

    start_time = time.time()


    try:

        response = await call_next(request)

        process_time = time.time() - start_time


        # Add headers

        response.headers["X-Process-Time"] = f"{process_time:.4f}"

        response.headers["X-Request-ID"] = str(id(request))


        # Log request

        logger.info(

            f"{request.method} {request.url.path} - "

            f"Status: {response.status_code} - "

            f"Time: {process_time:.4f}s"

        )


        return response


    except Exception as e:

        logger.error(f"Request failed: {str(e)}", exc_info=True)

        return JSONResponse(

            status_code=500,

            content={

                "success": False,

                "error": "Internal server error",

                "message": str(e),

            },

        )


# ============================================================================

# EXCEPTION HANDLERS

# ============================================================================


@app.exception_handler(404)

async def not_found_handler(request: Request, exc):

    """Handle 404 errors"""

    return JSONResponse(

        status_code=404,

        content={

            "success": False,

            "error": "Not Found",

            "message": f"The requested resource '{request.url.path}' was not found",

            "timestamp": datetime.utcnow().isoformat(),

        },

    )


@app.exception_handler(500)

async def server_error_handler(request: Request, exc):

    """Handle 500 errors"""

    logger.error(f"Internal server error: {exc}", exc_info=True)

    return JSONResponse(

        status_code=500,

        content={

            "success": False,

            "error": "Internal Server Error",

            "message": "An unexpected error occurred",

            "timestamp": datetime.utcnow().isoformat(),

        },

    )


@app.exception_handler(RequestValidationError)

async def validation_error_handler(request: Request, exc: RequestValidationError):

    """Handle validation errors"""

    return JSONResponse(

        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,

        content={

            "success": False,

            "error": "Validation Error",

            "details": exc.errors(),

            "body": exc.body,

            "timestamp": datetime.utcnow().isoformat(),

        },

    )


# ============================================================================

# PROMETHEUS METRICS (Optional)

# ============================================================================


if PROMETHEUS_AVAILABLE:

    Instrumentator().instrument(app).expose(app, endpoint="/metrics")

    logger.info("üìä Prometheus metrics enabled at /metrics")


def register_router(router, prefix: str, tags: list[str], router_name: str) -> None:



    try:

        app.include_router(router, prefix=prefix, tags=tags)

        logger.info(f"[OK] {router_name} routes registered successfully")

    except Exception as e:

        logger.error(

            f"[ERROR] Failed to register {router_name} routes: {str(e)}",

            exc_info=True,

            stack_info=True,

        )

        # Re-raise critical errors to prevent silent failures

        if isinstance(e, (ImportError, AttributeError)):

            raise


# Register all routes with consistent error handling
if ROUTES_AVAILABLE:
    # Define route configurations with proper typing - ONLY unified endpoint per constraints
    route_configs = [
        (analysis_router, "/api/v2/analysis", ["Analysis"], "Analysis"),  # Only unified routes
        (health_router, "/api/v2/health", ["Health"], "Health"),
    ]

    # Register all routes with consistent error handling
    for router, prefix, tags, name in route_configs:
        register_router(router, prefix, tags, name)


# ============================================================================

# WEBSOCKET ENDPOINT (Removed - handled by Node.js backend)

# WebSocket communication is handled by Node.js gateway at /api/v2/dashboard/ws

# Python focuses on ML computation only

# ============================================================================


@app.post("/test/analyze")
async def test_analyze_endpoint(file: UploadFile = File(...)):
    """Test endpoint that bypasses all authentication and uses real ML models"""
    try:
        import numpy as np
        from PIL import Image
        import io
        
        # Read file content
        content = await file.read()
        
        # Convert to PIL Image
        image = Image.open(io.BytesIO(content))
        
        # Convert to RGB if needed
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Convert to numpy array
        image_array = np.array(image)
        
        # Use real ML models if available
        if ML_AVAILABLE and hasattr(app.state, 'sentinel_agent') and app.state.sentinel_agent:
            try:
                # Use real SentinelAgent for analysis
                sentinel = app.state.sentinel_agent
                
                if hasattr(sentinel, 'image_detector') and sentinel.image_detector:
                    # Real ML inference
                    start_time = time.time()
                    
                    # Use the image detector
                    result_data = sentinel.image_detector.analyze_image(image_array)
                    
                    analysis_time = time.time() - start_time
                    
                    # Extract results
                    is_deepfake = result_data.get('is_deepfake', False)
                    confidence = result_data.get('confidence', 0.0)
                    model_name = result_data.get('model_name', 'unknown')
                    
                    result = {
                        "success": True,
                        "filename": file.filename,
                        "is_deepfake": is_deepfake,
                        "confidence": float(confidence),
                        "model_name": model_name,
                        "analysis_time": f"{analysis_time:.2f}s",
                        "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                        "message": "Real ML analysis completed - auth bypassed",
                        "ml_model": "sentinel_agent",
                        "image_shape": image_array.shape
                    }
                    
                    logger.info(f"‚úÖ Real ML analysis completed for {file.filename} - Deepfake: {is_deepfake}, Confidence: {confidence:.3f}")
                    return result
                else:
                    logger.warning("‚ö†Ô∏è Image detector not available in SentinelAgent")
                    
            except Exception as ml_error:
                logger.error(f"‚ùå ML model error: {str(ml_error)}")
                # Fallback to mock response
        else:
            logger.warning("‚ö†Ô∏è ML models not available, using mock response")
        
        # Fallback mock response (same as before)
        result = {
            "success": True,
            "filename": file.filename,
            "is_deepfake": False,  # Default to real for testing
            "confidence": 0.85,
            "model_name": "fallback_detector",
            "analysis_time": "0.5s",
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "message": "Fallback analysis completed - ML models unavailable",
            "ml_model": "mock"
        }
        
        logger.info(f"‚úÖ Test analysis completed for {file.filename}")
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Test analysis error: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "filename": file.filename if file else "unknown",
            "message": "Analysis failed"
        }


@app.get("/")

async def root():

    """Root endpoint"""

    return {

        "name": "SatyaAI API",

        "version": "2.0.0",

        "description": "Python-First ML/DL/NLP Deepfake Detection API",

        "status": "running",

        "timestamp": datetime.utcnow().isoformat(),

        "endpoints": {

            "docs": "/api/docs",

            "redoc": "/api/redoc",

            "health": "/api/health",

            "metrics": "/metrics" if PROMETHEUS_AVAILABLE else None,

        },

        "ml_models": {

            "image_detector": ML_AVAILABLE,

            "video_detector": ML_AVAILABLE,

            "audio_detector": ML_AVAILABLE,

            "text_nlp_detector": ML_AVAILABLE,

            "multimodal_fusion": ML_AVAILABLE,

        },

    }


@app.get("/health")

async def health_check():

    """Simple health check"""

    return {

        "status": "healthy",

        "version": "2.0.0",

        "timestamp": datetime.utcnow().isoformat(),

        "uptime": time.process_time(),

        "ml_models_loaded": ML_AVAILABLE,

        "database_connected": DB_AVAILABLE,

    }


# ===========================================================================

@app.get("/api/v2/models/status")

async def models_status():

    """Get status of all ML models"""

    try:

        model_info = {}

        
        if ML_AVAILABLE and hasattr(app.state, 'sentinel_agent') and app.state.sentinel_agent:

            try:

                # Get model info from SentinelAgent

                sentinel = app.state.sentinel_agent

                model_info = {

                    "image": {

                        "available": hasattr(sentinel, 'image_detector') and sentinel.image_detector is not None,

                        "weights": "efficientnet_b7" if hasattr(sentinel, 'image_detector') and sentinel.image_detector else None,

                        "device": "cuda" if torch.cuda.is_available() else "cpu"

                    },

                    "video": {

                        "available": hasattr(sentinel, 'video_detector') and sentinel.video_detector is not None,

                        "weights": "xception" if hasattr(sentinel, 'video_detector') and sentinel.video_detector else None,

                        "device": "cuda" if torch.cuda.is_available() else "cpu"

                    },

                    "audio": {

                        "available": hasattr(sentinel, 'audio_detector') and sentinel.audio_detector is not None,

                        "weights": "custom_audio" if hasattr(sentinel, 'audio_detector') and sentinel.audio_detector else None,

                        "device": "cpu"

                    },

                    "text": {

                        "available": hasattr(sentinel, 'text_nlp_detector') and sentinel.text_nlp_detector is not None,

                        "weights": "bert" if hasattr(sentinel, 'text_nlp_detector') and sentinel.text_nlp_detector else None,

                        "device": "cpu"

                    },

                    "multimodal": {

                        "available": hasattr(sentinel, 'multimodal_fusion_detector') and sentinel.multimodal_fusion_detector is not None,

                        "weights": "fusion" if hasattr(sentinel, 'multimodal_fusion_detector') and sentinel.multimodal_fusion_detector else None,

                        "device": "cuda" if torch.cuda.is_available() else "cpu"

                    }

                }

            except Exception as e:

                logger.error(f"Error getting model info from SentinelAgent: {e}")

                model_info = {

                    "image": {"available": False, "weights": "error", "device": "cpu"},

                    "video": {"available": False, "weights": "error", "device": "cpu"},

                    "audio": {"available": False, "weights": "error", "device": "cpu"},

                    "text": {"available": False, "weights": "error", "device": "cpu"},

                    "multimodal": {"available": False, "weights": "error", "device": "cpu"}

                }

        else:

            model_info = {

                "image": {"available": False, "weights": None, "device": "cpu"},

                "video": {"available": False, "weights": None, "device": "cpu"},

                "audio": {"available": False, "weights": None, "device": "cpu"},

                "text": {"available": False, "weights": None, "device": "cpu"},

                "multimodal": {"available": False, "weights": None, "device": "cpu"}

            }

        
        return {

            "success": True,

            "models": model_info,

            "ml_available": ML_AVAILABLE,

            "torch_version": torch.__version__ if ML_AVAILABLE else None,

            "cuda_available": torch.cuda.is_available() if ML_AVAILABLE else False

        }

    except Exception as e:

        logger.error(f"Models status endpoint failed: {e}")

        return {

            "success": False,

            "error": "Failed to get model status",

            "models": {

                "image": {"available": False, "weights": "error", "device": "cpu"},

                "video": {"available": False, "weights": "error", "device": "cpu"},

                "audio": {"available": False, "weights": "error", "device": "cpu"}

            }

        }


@app.get("/health/wiring")

async def wiring_check():

    """Return a list of all user‚Äëexposed routes for wiring verification.

    Internal FastAPI docs routes are filtered out.

    """

    routes = []

    for route in app.routes:

        if (

            route.path.startswith("/docs")

            or route.path.startswith("/redoc")

            or route.path.startswith("/openapi.json")

        ):

            continue

        methods = list(getattr(route, "methods", []))

        routes.append({"path": route.path, "methods": methods})

    return {"routes": routes}


@app.post("/api/v2/analysis/unified/image")

async def analyze_image_unified(

    file: UploadFile = File(...),

    analyze_forensics: bool = True,

    current_user: Optional[Dict] = Depends(get_current_user) if MIDDLEWARE_AVAILABLE else None

):

    """

    Unified image analysis with consistent interface.

    
    Args:

        file: Image file to analyze

        analyze_forensics: Whether to perform forensic analysis

        current_user: Authenticated user (if auth enabled)

        
    Returns:

        Standardized analysis result

    """

    # Check unified detector availability, fallback to SentinelAgent if not available
    if not UNIFIED_DETECTOR_AVAILABLE or not hasattr(app.state, 'unified_detector') or app.state.unified_detector is None:
        logger.warning("‚ö†Ô∏è Unified detector not available, using SentinelAgent fallback")
        
        # Fallback to SentinelAgent if unified detector not available
        if ML_AVAILABLE and hasattr(app.state, 'sentinel_agent') and app.state.sentinel_agent:
            try:
                # Read and validate file
                contents = await file.read()
                if not contents or len(contents) < 100:
                    raise ValueError("Invalid file")
                
                # Use SentinelAgent for analysis
                sentinel = app.state.sentinel_agent
                
                if hasattr(sentinel, 'image_detector') and sentinel.image_detector:
                    import numpy as np
                    from PIL import Image
                    import io
                    
                    # Convert to PIL Image
                    image = Image.open(io.BytesIO(contents))
                    if image.mode != 'RGB':
                        image = image.convert('RGB')
                    image_array = np.array(image)
                    
                    # Use image detector
                    result_data = sentinel.image_detector.analyze_image(image_array)
                    
                    # Convert to unified format
                    result_dict = {
                        "filename": file.filename,
                        "file_size": len(contents),
                        "user_id": current_user.get("id") if current_user else "anonymous",
                        "authenticity": "deepfake" if result_data.get('is_deepfake', False) else "authentic",
                        "confidence": float(result_data.get('confidence', 0.0)),
                        "is_deepfake": result_data.get('is_deepfake', False),
                        "model_name": result_data.get('model_name', 'sentinel_agent'),
                        "success": True
                    }
                    
                    logger.info(f"SentinelAgent fallback image analysis: {result_dict['authenticity']} ({result_dict['confidence']:.2f})")
                    
                    return {
                        "success": True,
                        "result": result_dict
                    }
                else:
                    raise HTTPException(status_code=503, detail="Image detector not available in SentinelAgent")
                    
            except Exception as e:
                logger.error(f"SentinelAgent fallback analysis failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        else:
            raise HTTPException(
                status_code=503,
                detail="ML models not available - please try again later"
            )

    
    try:

        # Read and validate file

        contents = await file.read()

        if not contents or len(contents) < 100:

            raise ValueError("Invalid file")

        
        # Perform unified analysis

        detector = app.state.unified_detector

        result = detector.detect_image(contents, analyze_forensics=analyze_forensics)

        
        # Add metadata

        result_dict = result.to_dict()

        result_dict.update({

            "filename": file.filename,

            "file_size": len(contents),

            "user_id": current_user.get("id") if current_user else "anonymous"

        })

        
        logger.info(f"Unified image analysis: {result.authenticity} ({result.confidence:.2f})")

        return {

            "success": result.success,

            "result": result_dict

        }

        
    except Exception as e:

        logger.error(f"Unified image analysis failed: {e}")

        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v2/analysis/unified/video")

async def analyze_video_unified(

    file: UploadFile = File(...),

    analyze_frames: Optional[int] = None,

    current_user: Optional[Dict] = Depends(get_current_user) if MIDDLEWARE_AVAILABLE else None

):

    """

    Unified video analysis with consistent interface.

    
    Args:

        file: Video file to analyze

        analyze_frames: Number of frames to analyze

        current_user: Authenticated user (if auth enabled)

        
    Returns:

        Standardized analysis result

    """

    # Check unified detector availability, fallback to SentinelAgent if not available
    if not UNIFIED_DETECTOR_AVAILABLE or not hasattr(app.state, 'unified_detector') or app.state.unified_detector is None:
        logger.warning("‚ö†Ô∏è Unified detector not available, using SentinelAgent fallback")
        
        # Fallback to SentinelAgent if unified detector not available
        if ML_AVAILABLE and hasattr(app.state, 'sentinel_agent') and app.state.sentinel_agent:
            try:
                # Read and validate file
                contents = await file.read()
                if not contents or len(contents) < 100:
                    raise ValueError("Invalid file")
                
                # Use SentinelAgent for analysis
                sentinel = app.state.sentinel_agent
                
                if hasattr(sentinel, 'video_detector') and sentinel.video_detector:
                    # Use video detector
                    result_data = sentinel.video_detector.analyze_video(contents)
                    
                    # Convert to unified format
                    result_dict = {
                        "filename": file.filename,
                        "file_size": len(contents),
                        "user_id": current_user.get("id") if current_user else "anonymous",
                        "authenticity": "deepfake" if result_data.get('is_deepfake', False) else "authentic",
                        "confidence": float(result_data.get('confidence', 0.0)),
                        "is_deepfake": result_data.get('is_deepfake', False),
                        "model_name": result_data.get('model_name', 'sentinel_agent'),
                        "success": True
                    }
                    
                    logger.info(f"SentinelAgent fallback video analysis: {result_dict['authenticity']} ({result_dict['confidence']:.2f})")
                    
                    return {
                        "success": True,
                        "result": result_dict
                    }
                else:
                    raise HTTPException(status_code=503, detail="Video detector not available in SentinelAgent")
                    
            except Exception as e:
                logger.error(f"SentinelAgent fallback analysis failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        else:
            raise HTTPException(
                status_code=503,
                detail="ML models not available - please try again later"
            )

    
    try:

        # Read and validate file

        contents = await file.read()

        if not contents or len(contents) < 100:

            raise ValueError("Invalid file")

        
        # Perform unified analysis

        detector = app.state.unified_detector

        result = detector.detect_video(contents, analyze_frames=analyze_frames)

        
        # Add metadata

        result_dict = result.to_dict()

        result_dict.update({

            "filename": file.filename,

            "file_size": len(contents),

            "user_id": current_user.get("id") if current_user else "anonymous"

        })

        
        logger.info(f"Unified video analysis: {result.authenticity} ({result.confidence:.2f})")

        return {

            "success": result.success,

            "result": result_dict

        }

        
    except Exception as e:

        logger.error(f"Unified video analysis failed: {e}")

        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/v2/analysis/unified/audio")

async def analyze_audio_unified(

    file: UploadFile = File(...),

    current_user: Optional[Dict] = Depends(get_current_user) if MIDDLEWARE_AVAILABLE else None

):

    """

    Unified audio analysis with consistent interface.

    
    Args:
        file: Audio file to analyze
        current_user: Authenticated user (if auth enabled)
        
    Returns:
        Standardized analysis result
    """

    # Check unified detector availability, fallback to SentinelAgent if not available
    if not UNIFIED_DETECTOR_AVAILABLE or not hasattr(app.state, 'unified_detector') or app.state.unified_detector is None:
        logger.warning("‚ö†Ô∏è Unified detector not available, using SentinelAgent fallback")
        
        # Fallback to SentinelAgent if unified detector not available
        if ML_AVAILABLE and hasattr(app.state, 'sentinel_agent') and app.state.sentinel_agent:
            try:
                # Read and validate file
                contents = await file.read()
                if not contents or len(contents) < 100:
                    raise ValueError("Invalid file")
                
                # Use SentinelAgent for analysis
                sentinel = app.state.sentinel_agent
                
                if hasattr(sentinel, 'audio_detector') and sentinel.audio_detector:
                    # Use audio detector
                    result_data = sentinel.audio_detector.analyze_audio(contents)
                    
                    # Convert to unified format
                    result_dict = {
                        "filename": file.filename,
                        "file_size": len(contents),
                        "user_id": current_user.get("id") if current_user else "anonymous",
                        "authenticity": "deepfake" if result_data.get('is_deepfake', False) else "authentic",
                        "confidence": float(result_data.get('confidence', 0.0)),
                        "is_deepfake": result_data.get('is_deepfake', False),
                        "model_name": result_data.get('model_name', 'sentinel_agent'),
                        "success": True
                    }
                    
                    logger.info(f"SentinelAgent fallback audio analysis: {result_dict['authenticity']} ({result_dict['confidence']:.2f})")
                    
                    return {
                        "success": True,
                        "result": result_dict
                    }
                else:
                    raise HTTPException(status_code=503, detail="Audio detector not available in SentinelAgent")
                    
            except Exception as e:
                logger.error(f"SentinelAgent fallback analysis failed: {e}")
                raise HTTPException(status_code=500, detail=str(e))
        else:
            raise HTTPException(
                status_code=503,
                detail="ML models not available - please try again later"
            )
    
    try:
        # Read and validate file
        contents = await file.read()
        if not contents or len(contents) < 100:
            raise ValueError("Invalid file")
        
        # Perform unified analysis
        detector = app.state.unified_detector
        result = detector.detect_audio(contents)
        
        # Add metadata
        result_dict = result.to_dict()
        result_dict.update({
            "filename": file.filename,
            "file_size": len(contents),
            "user_id": current_user.get("id") if current_user else "anonymous"
        })
        
        logger.info(f"Unified audio analysis: {result.authenticity} ({result.confidence:.2f})")
        
        return {
            "success": result.success,
            "result": result_dict
        }
        
    except Exception as e:
        logger.error(f"Unified audio analysis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/debug/analysis-status")
async def debug_analysis_status():
    """Debug endpoint to check analysis system status"""
    status = {
        "ml_available": ML_AVAILABLE,
        "db_available": DB_AVAILABLE,
        "middleware_available": MIDDLEWARE_AVAILABLE,
        "unified_detector_available": UNIFIED_DETECTOR_AVAILABLE,
        "sentinel_agent_available": hasattr(app.state, 'sentinel_agent') and app.state.sentinel_agent is not None,
        "models": {}
    }
    
    # Check individual model status
    if hasattr(app.state, 'sentinel_agent') and app.state.sentinel_agent:
        sentinel = app.state.sentinel_agent
        status["models"] = {
            "image_detector": hasattr(sentinel, 'image_detector') and sentinel.image_detector is not None,
            "video_detector": hasattr(sentinel, 'video_detector') and sentinel.video_detector is not None,
            "audio_detector": hasattr(sentinel, 'audio_detector') and sentinel.audio_detector is not None,
            "text_nlp_detector": hasattr(sentinel, 'text_nlp_detector') and sentinel.text_nlp_detector is not None,
        }
    
    return status


@app.get("/api/v2/analysis/unified/status")

async def get_unified_detector_status():

    """

    Get the status of the unified detector.

    
    Returns:

        Status information about the unified detector and available modalities

    """

    if not UNIFIED_DETECTOR_AVAILABLE:

        return {

            "available": False,

            "reason": "Unified detector not available"

        }

    
    if not hasattr(app.state, 'unified_detector') or app.state.unified_detector is None:

        return {

            "available": False,

            "reason": "Unified detector not initialized"

        }

    
    try:

        detector = app.state.unified_detector

        return {

            "available": True,

            "detector_info": detector.get_detector_info(),

            "performance_metrics": detector.get_performance_metrics()

        }

    except Exception as e:

        logger.error(f"Failed to get unified detector status: {e}")

        return {

            "available": False,

            "reason": f"Status check failed: {str(e)}"

        }


@app.post("/api/v2/analysis/multimodal")

async def analyze_multimodal(

    request: Request,

    current_user: Optional[Dict] = Depends(get_current_user) if MIDDLEWARE_AVAILABLE else None

):

   
    if not UNIFIED_DETECTOR_AVAILABLE or not hasattr(app.state, 'unified_detector') or app.state.unified_detector is None:

        raise HTTPException(

            status_code=503,

            detail="Unified detector not available"

        )

    
    try:

        # Get form data

        form = await request.form()

        
        # Read media files from form data

        image_files = []

        audio_files = []

        video_files = []

        
        # Extract files from form data

        for key, value in form.items():

            if isinstance(value, UploadFile):

                content = await value.read()

                if not content or len(content) < 100:

                    continue

                    
                # Determine file type by content and extension

                filename = value.filename or ""

                if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.gif')) or value.content_type.startswith('image/'):

                    image_files.append(content)

                elif filename.lower().endswith(('.mp3', '.wav', '.ogg', '.m4a', '.webm')) or value.content_type.startswith('audio/'):

                    audio_files.append(content)

                elif filename.lower().endswith(('.mp4', '.avi', '.mov', '.webm', '.mpeg')) or value.content_type.startswith('video/'):

                    video_files.append(content)

        
        # Check if at least one modality is provided

        if not any([image_files, audio_files, video_files]):

            raise HTTPException(

                status_code=400,

                detail="At least one media file must be provided"

            )

        
        # Perform multi-modal analysis using unified detector

        detector = app.state.unified_detector

        
        # Analyze each modality and combine results

        results = []

        
        # Analyze images

        for image_buffer in image_files:

            result = detector.detect_image(image_buffer)

            results.append(result.to_dict())

        
        # Analyze audio files

        for audio_buffer in audio_files:

            result = detector.detect_audio(audio_buffer)

            results.append(result.to_dict())

        
        # Analyze video files

        for video_buffer in video_files:

            result = detector.detect_video(video_buffer)

            results.append(result.to_dict())

        
        # Combine results (simple fusion - take average confidence and majority vote for authenticity)

        if results:

            combined_confidence = sum(r.get('confidence', 0) for r in results) / len(results)

            authentic_count = sum(1 for r in results if r.get('is_deepfake') == False)

            is_authentic = authentic_count > len(results) / 2

            
            combined_result = {

                "success": True,

                "authenticity": "authentic" if is_authentic else "deepfake",

                "confidence": combined_confidence,

                "is_deepfake": not is_authentic,

                "model_name": "SatyaAI-Multimodal",

                "model_version": "1.0.0",

                "summary": {

                    "total_files": len(results),

                    "images": len(image_files),

                    "audio_files": len(audio_files),

                    "video_files": len(video_files),

                    "individual_results": results

                },

                "user_id": current_user.get("id") if current_user else "anonymous"

            }

        else:

            combined_result = {

                "success": False,

                "error": "No valid files processed"

            }

        
        logger.info(f"Multimodal analysis completed: {combined_result.get('success', False)}")

        return {

            "success": combined_result.get("success", False),

            "result": combined_result

        }

        
    except Exception as e:

        logger.error(f"Multimodal analysis failed: {e}")

        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v2/analysis/multimodal/status")

async def get_multimodal_status():

    """

    Get the status of multi-modal fusion engine.

    
    Returns:

        Status information about available modalities and fusion engine

    """

    if not UNIFIED_DETECTOR_AVAILABLE:

        return {

            "available": False,

            "reason": "Unified detector not available",

            "modalities": {"image": False, "audio": False, "video": False}

        }

    
    if not hasattr(app.state, 'unified_detector') or app.state.unified_detector is None:

        return {

            "available": False,

            "reason": "Unified detector not initialized",

            "modalities": {"image": False, "audio": False, "video": False}

        }

    
    try:

        detector = app.state.unified_detector

        detector_info = detector.get_detector_info()

        return {

            "available": True,

            "detector_info": detector_info,

            "modalities": {

                "image": detector_info.get("image_available", False),

                "audio": detector_info.get("audio_available", False), 

                "video": detector_info.get("video_available", False)

            }

        }

    except Exception as e:

        logger.error(f"Failed to get multimodal status: {e}")

        return {

            "available": False,

            "reason": f"Status check failed: {str(e)}",

            "modalities": {"image": False, "audio": False, "video": False}

        }


@app.get("/api/v2/database/status")

async def get_database_status():

    """

    Get the status of Supabase database connection.

    
    Returns:

        Database connection status and configuration details

    """

    try:

        from database.base import validate_supabase_connection

        status = validate_supabase_connection()

        
        if status["status"] == "connected":

            return {

                "available": True,

                "database": "supabase",

                "status": status["status"],

                "message": status["message"],

                "key_type": status["key_type"],

                "configuration": {

                    "url_configured": status["url_configured"],

                    "key_configured": status["key_configured"]

                }

            }

        else:

            return {

                "available": False,

                "database": "supabase",

                "status": status["status"],

                "message": status["message"],

                "key_type": status.get("key_type", "unknown"),

                "error": status.get("error"),

                "configuration": {

                    "url_configured": status["url_configured"],

                    "key_configured": status["key_configured"]

                }

            }

            
    except Exception as e:

        logger.error(f"Failed to get database status: {e}")

        return {

            "available": False,

            "database": "supabase",

            "status": "error",

            "message": f"Status check failed: {str(e)}",

            "error": str(e)

        }


# WebSocket implementation removed - handled by Node.js backend

# All WebSocket communication should go through Node.js at /api/v2/dashboard/ws

# This keeps Python focused on ML computation only


# ===========================================================================

# CANONICAL API ENDPOINTS - Forward to /api/v2 routes

# ===========================================================================


@app.post("/analyze")

async def analyze_canonical(

    request: Request,

    current_user: Optional[Dict] = Depends(get_current_user) if MIDDLEWARE_AVAILABLE else None

):

    """

    Canonical analysis endpoint - auto-detects media type and routes appropriately.

    
    This is the main entry point for analysis requests.

    Automatically detects whether the uploaded file is image, video, or audio

    and forwards to the appropriate specialized endpoint.

    
    Args:

        request: FastAPI request object

        current_user: Authenticated user (if auth enabled)

        
    Returns:

        Analysis results from the appropriate detector

    """

    try:

        # Get form data

        form = await request.form()

        
        # Extract file from form

        file = None

        for key, value in form.items():

            if isinstance(value, UploadFile):

                file = value

                break

        
        if not file:

            raise HTTPException(status_code=400, detail="No file provided")

        
        # Read file content

        content = await file.read()

        if not content or len(content) < 100:

            raise HTTPException(status_code=400, detail="Invalid file")

        
        # Auto-detect file type

        import magic

        mime_type = magic.from_buffer(content, mime=True)

        
        logger.info(f"Canonical analysis: auto-detected type {mime_type} for file {file.filename}")

        
        # Route to appropriate endpoint based on MIME type

        if mime_type.startswith('image/'):

            # Forward to image analysis

            from fastapi import UploadFile

            from io import BytesIO

            
            # Create UploadFile object for forwarding

            file_obj = UploadFile(file.filename, BytesIO(content), content_type=mime_type)

            
            # Call the existing image analysis endpoint

            return await analyze_image_unified(file_obj, True, current_user)

            
        elif mime_type.startswith('video/'):

            # Forward to video analysis

            from fastapi import UploadFile

            from io import BytesIO

            
            file_obj = UploadFile(file.filename, BytesIO(content), content_type=mime_type)

            return await analyze_video_unified(file_obj, None, True, current_user)

            
        elif mime_type.startswith('audio/'):

            # Forward to audio analysis

            from fastapi import UploadFile

            from io import BytesIO

            
            file_obj = UploadFile(file.filename, BytesIO(content), content_type=mime_type)

            return await analyze_audio_unified(file_obj, current_user)

            
        else:

            raise HTTPException(

                status_code=400, 

                detail=f"Unsupported file type: {mime_type}. Supported types: image/*, video/*, audio/*"

            )

            
    except HTTPException:

        raise

    except Exception as e:

        logger.error(f"Canonical analysis failed: {e}")

        raise HTTPException(status_code=500, detail=str(e))


@app.post("/analyze/image")

async def analyze_image_canonical(

    file: UploadFile = File(...),

    analyze_forensics: bool = True,

    current_user: Optional[Dict] = Depends(get_current_user) if MIDDLEWARE_AVAILABLE else None

):

    """

    Canonical image analysis endpoint.

    
    Forwards requests to /api/v2/analysis/unified/image

    
    Args:

        file: Image file to analyze

        analyze_forensics: Whether to perform forensic analysis

        current_user: Authenticated user (if auth enabled)

        
    Returns:

        Image analysis results

    """

    logger.info(f"Canonical image analysis: {file.filename}")

    return await analyze_image_unified(file, analyze_forensics, current_user)


@app.post("/analyze/video")

async def analyze_video_canonical(

    file: UploadFile = File(...),

    analyze_frames: Optional[int] = None,

    analyze_forensics: bool = True,

    current_user: Optional[Dict] = Depends(get_current_user) if MIDDLEWARE_AVAILABLE else None

):

    """

    Canonical video analysis endpoint.

    
    Forwards requests to /api/v2/analysis/unified/video

    
    Args:

        file: Video file to analyze

        analyze_frames: Number of frames to analyze (None for auto)

        analyze_forensics: Whether to perform forensic analysis

        current_user: Authenticated user (if auth enabled)

        
    Returns:

        Video analysis results

    """

    logger.info(f"Canonical video analysis: {file.filename}")

    return await analyze_video_unified(file, analyze_frames, analyze_forensics, current_user)


@app.post("/analyze/audio")

async def analyze_audio_canonical(

    file: UploadFile = File(...),

    current_user: Optional[Dict] = Depends(get_current_user) if MIDDLEWARE_AVAILABLE else None

):

    """

    Canonical audio analysis endpoint.

    
    Forwards requests to /api/v2/analysis/unified/audio

    
    Args:

        file: Audio file to analyze

        current_user: Authenticated user (if auth enabled)

        
    Returns:

        Audio analysis results

    """

    logger.info(f"Canonical audio analysis: {file.filename}")

    return await analyze_audio_unified(file, current_user)


@app.get("/analyze/info")

async def analyze_get_info():

    """

    GET endpoint for /analyze - returns API information.

    
    Returns:

        Information about available analysis endpoints and supported formats

    """

    return {

        "endpoint": "/analyze",

        "method": "POST",

        "description": "Canonical analysis endpoint - auto-detects media type",

        "supported_formats": {

            "image": ["image/jpeg", "image/png", "image/webp", "image/gif"],

            "video": ["video/mp4", "video/avi", "video/mov", "video/webm"],

            "audio": ["audio/wav", "audio/mp3", "audio/m4a", "audio/ogg"]

        },

        "alternative_endpoints": {

            "image": "/analyze/image",

            "video": "/analyze/video", 

            "audio": "/analyze/audio"

        },

        "documentation": "Auto-detects file type and routes to appropriate specialized detector"

    }


if __name__ == "__main__":

    import uvicorn


    from config import settings

    
    uvicorn.run(

        "main_api:app",

        host=settings.HOST,

        port=settings.PORT,

        reload=settings.APP_ENV == "development",

        log_level="info",

        access_log=True,

    )

